# -*- coding: utf-8 -*-
"""Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZolA0bzhKXVmXGMzwobs5CWMn2g_h4wz
"""

import torch.nn as nn
from torch.nn.modules.conv import Conv2d
from torch.nn.modules.conv import Conv3d
    
class vggLike_3DCNN(nn.Module):
    def __init__(self, image, num_of_class):
        super(vggLike_3DCNN, self).__init__()
        self.features = self._make_layers()
        self.avgpool = nn.AdaptiveAvgPool3d((2, 2, 2))
        self.classifier = nn.Sequential(
            nn.Linear(512 * 2 * 2 * 2, 1024),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(1024, 1024),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(1024, num_of_class),
        )

        self._initialize_weights()

    def forward(self, x):
        x = self.features(x) # input:(batch_size, 1, Spec, Height, Width) output:(batch_size, 512, /32, /32, /32)
        x = self.avgpool(x) # output:(batch_size, 512, 2, 2, 2)
        x = x.reshape(x.shape[0], -1) # output:(batch_size, 4096)
        x = self.classifier(x) # output:(batch_size, num_of_class)

        return x

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def _make_layers(self):
        layers = []
        in_channels = 1
        
        # VGG16
        cfg = [64, 64, "M", 128, 128, "M", 256, 256, 256, "M", 512, 512, 512, "M", 512, 512, 512, "M"]
        
        # VGG19
        # cfg = [64, 64, "M", 128, 128, "M", 256, 256, 256, 256, "M", 512, 512, 512, 512, "M", 512, 512, 512, 512, "M"]
        for v in cfg:
            if v == "M":
                layers += [nn.MaxPool3d(kernel_size=2, stride=2)]
            else:
                conv3d = nn.Conv3d(in_channels, v, kernel_size=3, padding=1)
                layers += [conv3d, nn.BatchNorm3d(v), nn.ReLU(True)]
                in_channels = v

        return nn.Sequential(*layers)
    
class block(nn.Module):
    def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):
        """
        残差ブロックを作成するクラス
        Args:
            first_conv_in_channels : 1番目のconv層（1×1）のinput channel数
            first_conv_out_channels : 1番目のconv層（1×1）のoutput channel数
            identity_conv : channel数調整用のconv層
            stride : 3×3conv層におけるstide数。sizeを半分にしたいときは2に設定
        """        
        super(block, self).__init__()

        # 1番目のconv層（1×1）
        self.conv1 = nn.Conv3d(
            first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm3d(first_conv_out_channels)

        # 2番目のconv層（3×3）
        # パターン3の時はsizeを変更できるようにstrideは可変
        self.conv2 = nn.Conv3d(
            first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn2 = nn.BatchNorm3d(first_conv_out_channels)

        # 3番目のconv層（1×1）
        # output channelはinput channelの4倍になる
        self.conv3 = nn.Conv3d(
            first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)
        self.bn3 = nn.BatchNorm3d(first_conv_out_channels*4)
        self.relu = nn.ReLU()

        # identityのchannel数の調整が必要な場合はconv層（1×1）を用意、不要な場合はNone
        self.identity_conv = identity_conv

    def forward(self, x):

        identity = x.clone()  # 入力を保持する

        x = self.conv1(x)  # 1×1の畳み込み
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)  # 3×3の畳み込み（パターン3の時はstrideが2になるため、ここでsizeが半分になる）
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)  # 1×1の畳み込み
        x = self.bn3(x)

        # 必要な場合はconv層（1×1）を通してidentityのchannel数の調整してから足す
        if self.identity_conv is not None:
            identity = self.identity_conv(identity)
        x += identity

        x = self.relu(x)

        return x
    
class ResNetLike_3DCNN(nn.Module):
    def __init__(self, image, num_of_class):
        super(ResNetLike_3DCNN, self).__init__()

        # conv1はアーキテクチャ通りにベタ打ち
        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)

        # conv2_xはサイズの変更は不要のため、strideは1
        self.conv2_x = self._make_layer(3, res_block_in_channels=64, first_conv_out_channels=64, stride=1)

        # conv3_x以降はサイズの変更をする必要があるため、strideは2
        self.conv3_x = self._make_layer(4, res_block_in_channels=256,  first_conv_out_channels=128, stride=2)
        self.conv4_x = self._make_layer(6, res_block_in_channels=512,  first_conv_out_channels=256, stride=2)
        self.conv5_x = self._make_layer(3, res_block_in_channels=1024, first_conv_out_channels=512, stride=2)

        self.avgpool = nn.AdaptiveAvgPool3d((1,1,1))
        self.fc = nn.Linear(2048, num_of_class)

    def forward(self,x):
        x = self.conv1(x)   # in:(batch_size,1,Spec*Height*Width)、out:(batch_size,64,/2*/2*/2)
        x = self.bn1(x)     # out:(batch_size,64,/2*/2*/2)
        x = self.relu(x)    # out:(batch_size,64,/2*/2*/2)
        x = self.maxpool(x) # out:(batch_size,64,/4*/4*/4)

        x = self.conv2_x(x)  # out:(batch_size,256,/4*/4*/4)
        x = self.conv3_x(x)  # out:(batch_size,512,/8*/8*/8)
        x = self.conv4_x(x)  # out:(batch_size,1024,/16*/16*/16)
        x = self.conv5_x(x)  # out:(batch_size,2048,/32*/32*/32)
        x = self.avgpool(x) # out:(batch_size,2048,1,1,1)
        x = x.reshape(x.shape[0], -1) # out:(batch_size,2048*1*1*1)
        x = self.fc(x) # out:(batch_size,num_of_class)

        return x

    def _make_layer(self, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):
        layers = []

        # 1つ目の残差ブロックではchannel調整、及びsize調整が発生する
        # identifyを足す前に1×1のconv層を追加し、サイズ調整が必要な場合はstrideを2に設定
        identity_conv = nn.Conv3d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)
        layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))

        # 2つ目以降のinput_channel数は1つ目のoutput_channelの4倍
        in_channels = first_conv_out_channels*4

        # channel調整、size調整は発生しないため、identity_convはNone、strideは1
        for i in range(num_res_blocks - 1):
            layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))

        return nn.Sequential(*layers)
    
class my_3DCNN(nn.Module):
    def __init__(self, num_of_class):
        super(my_3DCNN, self).__init__()
        
        self.relu = nn.ReLU()
        self.conv1D_layers = nn.Sequential(
            nn.Conv3d(1, 2, kernel_size=(7,1,1), padding=(3,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU(),
            nn.Conv3d(2, 4, kernel_size=(7,1,1), padding=(3,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU(),
            nn.Conv3d(4, 8, kernel_size=(5,1,1), padding=(2,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU(),
            nn.Conv3d(8, 16, kernel_size=(5,1,1), padding=(2,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU(),
            nn.Conv3d(16, 32, kernel_size=(3,1,1), padding=(1,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU(),
            nn.Conv3d(32, 64, kernel_size=(3,1,1), padding=(1,0,0)),
            nn.MaxPool3d((2,1,1)),
            nn.ReLU()
        )
        self.conv2D_layers = nn.Sequential(
            nn.Conv3d(64, 128, kernel_size=(1,9,9), padding=(0,4,4)),
            nn.MaxPool3d((1,2,2)),
            nn.ReLU()
        )
        self.avgpool = nn.AdaptiveAvgPool3d((3,16,16))
        self.fc1 = nn.Linear(128*3*16*16, 768)
        self.fc2 = nn.Linear(768, 768)
        self.fc3 = nn.Linear(768, num_of_class)
        
    def forward(self, x):
        x = self.conv1D_layers(x) # out:(batch_size,64,Spec/64*Height*Width)
        x = self.conv2D_layers(x) # out:(batch_size,128,Spec/64*Height/2*Width/2)
        x = self.avgpool(x) # out:(batch_size,128,3,16,16)
        x = x.view(x.size(0), -1) # out:(batch_size,128*3*16*16)
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        return x
    
    
class sample_3DCNN(nn.Module):
    def __init__(self, image, num_of_class):
        super(sample_3DCNN, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv3d(1, 8, kernel_size=3, padding=1),
            nn.MaxPool3d(2),
            nn.ReLU(),
            nn.Conv3d(8, 16, kernel_size=3, padding=1),
            nn.MaxPool3d(2),
            nn.ReLU(),
            nn.Conv3d(16, 32, kernel_size=3, padding=1),
            nn.MaxPool3d(2),
            nn.Dropout(),
            nn.ReLU(),
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(32*int((image.size(1)/8)*(image.size(2)/8)*(image.size(3)/8)), 32),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(32, num_of_class),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.conv_layers(x) # out:(batch_size,32,/8*/8*/8)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x

if __name__ == '__main__':
    import Choice_Model
    target_class = [0, 1]
    model_name = 'sample_3DCNN'
    model = Choice_Model(model_name, target_class)
    print(model)